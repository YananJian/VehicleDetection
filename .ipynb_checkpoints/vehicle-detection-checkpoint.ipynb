{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "\n",
    "1. I used Caffe to train a small neural net that can detect vehicles. <br/>\n",
    "2. My classifier's accuracy on the training set is 100%. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end training for vehicle detetion\n",
    "\n",
    "There are only 1050 images in the training set. Since data set is small, we can leverage a simple neural network architecture. \n",
    "The reason that I choose to use nerual network is that NN is scalable, compared to the HOG+SVM implementation, nerual networks can easily be scaled up to classify RGB images or detect more objects. \n",
    "There are various choices on the neural net frameworks, I'm using Caffe for this project, because Caffe is optimized on training images. \n",
    "\n",
    "For the visualization of NN and the thoughs on params of the NN, please refer to the 4th section(at the end of this file) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start with training the data. This involves the following parts:\n",
    "(1) Write all training image fileNames and related labels into a file.<br/>\n",
    "(2) Define the network architecture.<br/>\n",
    "(3) Start training.<br/>\n",
    "\n",
    "After training the net, we get a caffe model file: weights.vehicle.caffemodel. We then save it locally and use it to predict labels on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read image filenames and labels into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_imgs = glob.glob(\"images/test/*.jpg\")\n",
    "train_imgs = glob.glob(\"images/train/*.jpg\")\n",
    "\n",
    "with open(\"train.txt\", 'w') as outfile:\n",
    "    for f in train_imgs:\n",
    "        if 'pos-' in f:\n",
    "            outfile.write(f + \" \" + \"1\\n\" )\n",
    "        elif 'neg-' in f:\n",
    "            outfile.write(f + \" \" + \"0\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Defining and running the nets\n",
    "\n",
    "We'll start by defining `build_net`, a function which initializes the vehicle net architecture (a minor variant on *LeNet*), taking arguments specifying the data and number of output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "weight_param = dict(lr_mult=1, decay_mult=1)\n",
    "bias_param   = dict(lr_mult=2, decay_mult=0)\n",
    "learned_param = [weight_param, bias_param]\n",
    "frozen_param = [dict(lr_mult=0)] * 2\n",
    "\n",
    "def conv(bottom, ks, nout, stride=1, pad=0, group=1,\n",
    "              param=learned_param,\n",
    "              weight_filler=dict(type='xavier', std=0.01),\n",
    "              bias_filler=dict(type='constant', value=0.1)):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "                         num_output=nout, pad=pad, group=group,\n",
    "                         param=param, weight_filler=weight_filler,\n",
    "                         bias_filler=bias_filler)\n",
    "    return conv\n",
    "\n",
    "def fc_relu(bottom, nout, param=learned_param,\n",
    "            weight_filler=dict(type='xavier', std=0.005),\n",
    "            bias_filler=dict(type='constant', value=0.1)):\n",
    "    fc = L.InnerProduct(bottom, num_output=nout, param=param,\n",
    "                        weight_filler=weight_filler,\n",
    "                        bias_filler=bias_filler)\n",
    "    return fc, L.ReLU(fc, in_place=True)\n",
    "\n",
    "def max_pool(bottom, ks, stride=1):\n",
    "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n",
    "\n",
    "def build_net(data, label=None, train=True, num_classes=2,\n",
    "             classifier_name='fc8', learn_all=False):\n",
    "    n = caffe.NetSpec()\n",
    "    n.data = data\n",
    "    param = learned_param if learn_all else frozen_param\n",
    "    n.conv1 = conv(n.data, 5, 4, stride=1, param=param)\n",
    "    n.pool1 = max_pool(n.conv1, 2, stride=2)\n",
    "    n.conv2 = conv(n.pool1, 5, 2, stride=1, param=param)\n",
    "    n.pool2 = max_pool(n.conv2, 2, stride=2)\n",
    "    ip1, relu1 = fc_relu(n.pool2, nout=10, param=learned_param)\n",
    "    ip2 = L.InnerProduct(ip1, num_output=2, param=learned_param)\n",
    "\n",
    "    if not train:\n",
    "        n.probs = L.Softmax(ip2)\n",
    "\n",
    "    if label is not None:\n",
    "        n.label = label\n",
    "        n.loss = L.SoftmaxWithLoss(ip2, n.label)\n",
    "        n.acc = L.Accuracy(ip2, n.label)\n",
    "    # write the net to a temporary file and return its filename\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(str(n.to_proto()))\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function `vehicle_net` which calls `build_net` on data from the training dataset.\n",
    "\n",
    "The new network will also have the vehicle net architecture:\n",
    "\n",
    "- the input is the vehicle training data we have in the dir images/train/, provided by an `ImageData` layer\n",
    "- the output is a distribution over 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vehicle_net(train=True, learn_all=False, subset=None, batch_size=1050):\n",
    "    if subset is None:\n",
    "        subset = 'train' if train else 'test'\n",
    "    source = '%s.txt' % subset\n",
    "    vehicle_data, vehicle_label = L.ImageData(\n",
    "        source=source,\n",
    "        batch_size=batch_size,  ntop=2, is_color=False, shuffle=False)\n",
    "    if train:\n",
    "        learn_all = learn_all\n",
    "        label = vehicle_label\n",
    "    else:\n",
    "        learn_all = False\n",
    "        label = None\n",
    "    return build_net(data=vehicle_data, label=label, train=train,\n",
    "                    num_classes=2,\n",
    "                    classifier_name='vehicle',\n",
    "                    learn_all=learn_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `vehicle_net` function defined above to initialize the vehicle net, with input images from the training dataset.\n",
    "\n",
    "Call `forward` to get a batch of vehicle training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_vehicle_net():\n",
    "\n",
    "    niter = 200  # number of iterations to train\n",
    "\n",
    "    vehicle_solver_filename = solver(vehicle_net(train=True))\n",
    "    vehicle_solver = caffe.get_solver(vehicle_solver_filename)\n",
    "\n",
    "    print 'Running solvers for %d iterations...' % niter\n",
    "    solvers = [('scratch', vehicle_solver)]\n",
    "    loss, acc, weights = run_solvers(niter, solvers)\n",
    "    print 'Done.'\n",
    "\n",
    "    train_loss = loss['scratch']\n",
    "    train_acc = acc['scratch']\n",
    "    vehicle_weights = weights['scratch']\n",
    "\n",
    "    print 'train loss:', train_loss\n",
    "    print 'train acc:', train_acc\n",
    "    print 'vehicle weights:', vehicle_weights\n",
    "\n",
    "def solver(train_net_path, test_net_path=None, base_lr=0.001):\n",
    "    s = caffe_pb2.SolverParameter()\n",
    "\n",
    "    s.train_net = train_net_path\n",
    "    if test_net_path is not None:\n",
    "        s.test_net.append(test_net_path)\n",
    "        s.test_interval = 100  # Test after every 100 training iterations.\n",
    "        s.test_iter.append(100) # Test on 100 batches each time we test.\n",
    "\n",
    "    # The number of iterations over which to average the gradient.\n",
    "    # Effectively boosts the training batch size by the given factor, without\n",
    "    # affecting memory utilization.\n",
    "    s.iter_size = 1\n",
    "    s.max_iter = 100000     # # of times to update the net (training iterations)\n",
    "\n",
    "    # Solve using the stochastic gradient descent (SGD) algorithm.\n",
    "    s.type = 'SGD'\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    s.base_lr = base_lr\n",
    "\n",
    "    s.lr_policy = 'inv'\n",
    "    s.gamma = 0.0001\n",
    "    s.power = 0.75\n",
    "\n",
    "    s.momentum = 0.9\n",
    "    s.weight_decay = 5e-4\n",
    "\n",
    "    # Display the current training loss and accuracy every 10 iterations.\n",
    "    s.display = 10\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.  Here, we'll\n",
    "    # snapshot every 10K iterations -- ten times during training\n",
    "    # -- as long as we have that much data to train.\n",
    "    s.snapshot = 10000\n",
    "    s.snapshot_prefix = caffe_root + 'models/finetune_vehicle/finetune_vehicle'\n",
    "\n",
    "    # Train on the GPU.  Using the CPU to train large networks is very slow.\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "\n",
    "    # Write the solver to a temporary file and return its filename.\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(str(s))\n",
    "        return f.name\n",
    "\n",
    "def run_solvers(niter, solvers, disp_interval=10):\n",
    "    blobs = ('loss', 'acc')\n",
    "    loss, acc = ({name: np.zeros(niter) for name, _ in solvers}\n",
    "                 for _ in blobs)\n",
    "    for it in range(niter):\n",
    "        for name, s in solvers:\n",
    "            s.step(1)  # run a single SGD step in Caffe\n",
    "            loss[name][it], acc[name][it] = (s.net.blobs[b].data.copy()\n",
    "                                             for b in blobs)\n",
    "            loss_disp = '; '.join('%s: loss=%.3f, acc=%2d%%' %\n",
    "                                  (n, loss[n][it], np.round(100*acc[n][it]))\n",
    "                                  for n, _ in solvers)\n",
    "            print '%3d) %s' % (it, loss_disp)\n",
    "    # Save the learned weights from nets.\n",
    "    weight_dir = tempfile.mkdtemp()\n",
    "    weights = {}\n",
    "    for name, s in solvers:\n",
    "        filename = 'weights.%s.caffemodel' % name\n",
    "        weights[name] = os.path.join(weight_dir, filename)\n",
    "        s.net.save(weights[name])\n",
    "    return loss, acc, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_solvers runs n numbers of iterations on solvers. Solver represents the network, runs it, saves it. I tried different solvers so that here in run_solvers it takes a list of solvers as param.\n",
    "\n",
    "After 200 iterations, the accuracy of the network is 100%...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running the classifier on test data\n",
    "\n",
    "Now, we'll run the classifier on the test data set and generate the result csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_vehicle_net(weights):\n",
    "    file_names = []\n",
    "    with open('test.txt', 'r') as sourcefile:\n",
    "        for line in sourcefile:\n",
    "            file_names.append(line.split(' ')[0])\n",
    "    batch_size = len(file_names)\n",
    "    test_net = caffe.Net(vehicle_net(train=False, batch_size=batch_size), weights, caffe.TEST)\n",
    "    test_net.forward()\n",
    "    data_batch = test_net.blobs['data'].data.copy()\n",
    "    classifications= []\n",
    "    for i in range(0, batch_size):\n",
    "        image = data_batch[i]\n",
    "        label = disp_vehicle_preds(test_net, image)\n",
    "        classifications.append([file_names[i].split('/')[-1], label])\n",
    "    with open('classifications.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(classifications)\n",
    "\n",
    "weights = 'weights.vehicle.caffemodel'\n",
    "eval_vehicle_net(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](vehicle_network_structure.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using only two layers of convolution, because the input dataset is small, the input pixel size is 40\\*100 for every image, so that there are 4000 input params. With kernel size 5 and output size as 4 for the first convolutional layer, we have (40-5+1)\\*(100-5+1)\\*4 = 36\\*96 params in the first convolutional layer. Same calculation applies for the next conv layer. We have to make sure that number of params adding together is smaller than number of input pixels (for all input training set images), or else we'll underfit."
   ]
  }
 ],
 "metadata": {
  "description": "Fine-tune the ImageNet-trained CaffeNet on new data.",
  "example_name": "Fine-tuning for Style Recognition",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "priority": 3
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
